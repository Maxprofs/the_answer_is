{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from gensim import corpora\n",
    "\n",
    "class MyDocument(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            with open(os.path.join(self.dirname, fname)) as content_file:\n",
    "                content = content_file.read()  \n",
    "                content = unicode(content, encoding='utf-8', errors='replace')\n",
    "                yield content.lower().split()\n",
    "\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                line = unicode(line, encoding='utf-8', errors='replace')\n",
    "                yield line.lower().split()\n",
    "\n",
    "def get_dictionary(path):\n",
    "    dictionary = corpora.Dictionary( MySentences(path) )\n",
    "    stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\n",
    "    dictionary.filter_tokens(stop_ids) # remove stop words and words that appear only once\n",
    "    dictionary.compactify() # remove gaps in id sequence after words that were removed\n",
    "    return dictionary\n",
    "\n",
    "def get_document_tuple( path, dictionary ):\n",
    "    vector = pd.Series()\n",
    "    doclist = MyDocument(path) \n",
    "    for i,content in enumerate(doclist):\n",
    "        vector = vector.set_value(i,dictionary.doc2bow(content))\n",
    "    return list(vector)\n",
    "\n",
    "def transform_tuple_into_vector(document_tuple,dictionary):\n",
    "    vector = pd.Series(np.zeros(len(dictionary.token2id)))        #0 array for storing wiki document vectors.\n",
    "    if not document_tuple == []:\n",
    "        for onetuple in document_tuple: \n",
    "            vector[onetuple[0]] = onetuple[1]\n",
    "        vector = vector / np.linalg.norm(vector, ord = 1)                       #normalize vector     \n",
    "        return vector \n",
    "    else:\n",
    "        return vector\n",
    "\n",
    "def transform_tuples_into_dataframe(document_tuples, dictionary):\n",
    "    length = len(document_tuples) \n",
    "    vector_initialization = pd.Series(np.zeros(len(dictionary.token2id)))  #0 array for storing wiki document vectors. \n",
    "    df_vector = pd.DataFrame(vector_initialization)  #initialize dataframe. all vectors will be stored. \n",
    "    for i in xrange(len(document_tuples)):           \n",
    "        #for each wiki documents, we will transform wiki vectors in tuple form into\n",
    "        #vectors in ususal form. \n",
    "        vector = transform_tuple_into_vector( document_tuples[i], dictionary )\n",
    "        df_vector[i]= pd.DataFrame(vector)\n",
    "    return df_vector\n",
    "\n",
    "def get_close_documents(string, dataframe, dictionary, topn):\n",
    "    #string = unicode(string, encoding='utf-8', errors='replace').lower()\n",
    "    string = string_stemmer(string)\n",
    "    #print string\n",
    "    first_vector = transform_tuple_into_vector(dictionary.doc2bow(string.split()), dictionary )\n",
    "    #print first_vector\n",
    "    lengthlist = pd.Series()\n",
    "    for j in xrange(len(dataframe.columns)):\n",
    "        #lengthlist = lengthlist.set_value(j, np.linalg.norm(first_vector-dataframe[j]) )\n",
    "        lengthlist = lengthlist.set_value( j, spatial.distance.cosine(first_vector, dataframe[j]))\n",
    "    return lengthlist.sort_values().head(topn)\n",
    "\n",
    "def get_document_by_index(path,index):\n",
    "    # this gets the filename and content of the document in a directory by index. \n",
    "    i= 0\n",
    "    for fname in os.listdir(path):\n",
    "        if i == index:\n",
    "            with open(os.path.join(path, fname)) as content_file:\n",
    "                content = content_file.read()  \n",
    "                content = unicode(content, encoding='utf-8', errors='replace')\n",
    "                return fname, content\n",
    "        i= i + 1\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_contents_of_close_documents_by_paragraph(path, close_documents_list):\n",
    "    merged = ''\n",
    "    for i in close_documents_list.index:\n",
    "        name, text = get_document_by_index(path,i)\n",
    "        merged = merged + '\\n' + text        \n",
    "    return merged.split('\\n')\n",
    "\n",
    "\n",
    "def get_distance_between_two_documents(A, B, dictionary):\n",
    "    import re\n",
    "    if A:\n",
    "        A = re.sub(r'[^a-zA-Z ]',r'',A).lower().split()\n",
    "    else:\n",
    "        A = ['the']        # just in case A, or B is empty. \n",
    "    A = dictionary.doc2bow(A)\n",
    "    A = transform_tuple_into_vector( A ,dictionary)\n",
    "    \n",
    "    if B:\n",
    "        B = re.sub(r'[^a-zA-Z ]',r'',B).lower().split()\n",
    "    else:\n",
    "        B = ['the']       # just in case A, or B is empty. \n",
    "    B = dictionary.doc2bow(B)\n",
    "    B = transform_tuple_into_vector( B ,dictionary)\n",
    "    #length = spatial.distance.cosine(A,B)\n",
    "    # I would like to use cosine distance, but the vectors are so sparse that most of the time the output is 0 .\n",
    "    # So we use euclidean distance. \n",
    "    length = np.linalg.norm(A-B)\n",
    "    return length\n",
    "\n",
    "\n",
    "def string_stemmer(line):\n",
    "    import re\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    line = re.sub(r'[^a-zA-Z ]',r'',line)\n",
    "    line = line.split()\n",
    "    line = [word for word in line if word not in stopwords.words('english')]  # remove the stop words. \n",
    "    output = []\n",
    "    for word in line:\n",
    "        output.append(stemmer.stem(word))     #stem all words \n",
    "    output = ' '.join(output)           # join the list to make a string\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_my_answer_all_distance(train, dictionary, df_vector, wiki_path):\n",
    "    correct = 0\n",
    "    convert_answer = {3: 'A', 4: 'B', 5: 'C', 6: 'D' }\n",
    "    myanswer = pd.Series()    #initialize dataframe to store my answers\n",
    "    myanswer_distance = pd.DataFrame(np.zeros(4).reshape(1,4), columns = ['A','B','C','D'])    #initialize dataframe to store my answers distance\n",
    "    for i in xrange(len(train)):       #loop through all questions\n",
    "        q = train.ix[i][1]\n",
    "        close_documents = get_close_documents(q, df_vector, dictionary,5)\n",
    "        merged = get_contents_of_close_documents_by_paragraph(wiki_path, close_documents)\n",
    "        four_choices = pd.Series()         # initialize a series to store the best value for each answer. \n",
    "        for j in [3,4,5,6]:            # the columns where the answer option lies. \n",
    "            A = train.ix[i][1] + ' ' + train.ix[i][j]       # question + each answer choice. \n",
    "            dist_list = []             # for storing all distance between A and all paragraphs in close documents. \n",
    "            for m in xrange(len(merged)):\n",
    "                distance = get_distance_between_two_documents(A, merged[m], dictionary)\n",
    "                if distance > 0:  #to disregrad nan value\n",
    "                    dist_list.append( distance )\n",
    "            four_choices = four_choices.set_value( j, min(dist_list)  )\n",
    "            #print np.std(dist_list)\n",
    "            #print min(dist_list) \n",
    "        myanswer_distance.set_value(i, 'A', four_choices[3] )\n",
    "        myanswer_distance.set_value(i, 'B', four_choices[4] )\n",
    "        myanswer_distance.set_value(i, 'C', four_choices[5] )\n",
    "        myanswer_distance.set_value(i, 'D', four_choices[6] )\n",
    "        myanswer = myanswer.set_value(i, convert_answer[ four_choices.argmin() ])\n",
    "        print 'question: ',q\n",
    "        print 'answer: ',train.ix[i][3], train.ix[i][4] ,train.ix[i][5], train.ix[i][6]\n",
    "        print 'correct answer: ', train.ix[i][2]\n",
    "        print i, four_choices[3], four_choices[4], four_choices[5], four_choices[6]\n",
    "        print 'my answer: ', convert_answer[ four_choices.argmin() ]\n",
    "        if train.ix[i][2] == convert_answer[ four_choices.argmin() ]:\n",
    "            correct = correct +1.0\n",
    "        print 'percent correct: ', correct / (i+1) \n",
    "    return myanswer_distance, myanswer\n",
    "\n",
    "\n",
    "\n",
    "def run_fetch_ws(train_file_path, dictionary_folder_path, wiki_folder_path):\n",
    "    dictionary = get_dictionary(dictionary_folder_path)\n",
    "    wiki_tuple = get_document_tuple( wiki_folder_path, dictionary )\n",
    "    df_wiki_vector = transform_tuples_into_dataframe(wiki_tuple,dictionary)  \n",
    "    train = pd.read_table(train_file_path,sep = '\\t')\n",
    "    distance, answer = get_my_answer_all_distance(train, dictionary, df_wiki_vector, wiki_folder_path)\n",
    "    \n",
    "    train['fetch_doc_ws_train_answer'] = answer\n",
    "    train['fetch_doc_ws_train_correct'] = (train['correctAnswer'] == train['fetch_doc_ws_train_answer'])\n",
    "    print 'percent correct is ' , train['fetch_doc_ws_train_correct'].sum(axis =0) / (len(train) + 0.0)\n",
    "    train.to_csv('/Users/MK/GitHub/the_answer_is/data/answer/fetch_doc_ws_train.csv', encoding='utf-8')\n",
    "    \n",
    "    return distance, answer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stoplist = [u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours',\n",
    "             u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', \n",
    "             u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', \n",
    "             u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', \n",
    "             u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', \n",
    "             u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', \n",
    "             u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', \n",
    "             u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', \n",
    "             u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', \n",
    "             u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', \n",
    "             u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', \n",
    "             u'should', u'now']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_wiki_folder_path = '/Users/MK/GitHub/the_answer_is/data/wikipedia_stemmed_all_merged'\n",
    "my_train_file_path = '/Users/MK/GitHub/the_answer_is/data/training_set.tsv'\n",
    "my_dictionary_folder_path = '/Users/MK/GitHub/the_answer_is/data/temporary2'  #this folder contains one file which is stemmed. \n",
    "distance, answer  = run_fetch_ws(my_train_file_path, my_dictionary_folder_path, my_wiki_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
