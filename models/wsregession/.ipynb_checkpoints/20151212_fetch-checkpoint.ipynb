{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from gensim import corpora\n",
    "\n",
    "class MyDocument(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            with open(os.path.join(self.dirname, fname)) as content_file:\n",
    "                content = content_file.read()  \n",
    "                content = unicode(content, encoding='utf-8', errors='replace')\n",
    "                yield content.lower().split()\n",
    "\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                line = unicode(line, encoding='utf-8', errors='replace')\n",
    "                yield line.lower().split()\n",
    "\n",
    "def get_dictionary(path):\n",
    "    dictionary = corpora.Dictionary( MySentences(path) )\n",
    "    stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\n",
    "    dictionary.filter_tokens(stop_ids) # remove stop words and words that appear only once\n",
    "    dictionary.compactify() # remove gaps in id sequence after words that were removed\n",
    "    return dictionary\n",
    "\n",
    "def get_document_tuple( path, dictionary ):\n",
    "    vector = pd.Series()\n",
    "    doclist = MyDocument(path) \n",
    "    for i,content in enumerate(doclist):\n",
    "        vector = vector.set_value(i,dictionary.doc2bow(content))\n",
    "    return list(vector)\n",
    "\n",
    "def transform_tuple_into_vector(document_tuple,dictionary):\n",
    "    vector = pd.Series(np.zeros(len(dictionary.token2id)))        #0 array for storing wiki document vectors.\n",
    "    if not document_tuple == []:\n",
    "        for onetuple in document_tuple: \n",
    "            vector[onetuple[0]] = onetuple[1]\n",
    "        vector = vector / np.linalg.norm(vector, ord = 1)                       #normalize vector     \n",
    "        return vector \n",
    "    else:\n",
    "        return vector\n",
    "\n",
    "def transform_tuples_into_dataframe(document_tuples, dictionary):\n",
    "    length = len(document_tuples) \n",
    "    vector_initialization = pd.Series(np.zeros(len(dictionary.token2id)))  #0 array for storing wiki document vectors. \n",
    "    df_vector = pd.DataFrame(vector_initialization)  #initialize dataframe. all vectors will be stored. \n",
    "    for i in xrange(len(document_tuples)):           \n",
    "        #for each wiki documents, we will transform wiki vectors in tuple form into\n",
    "        #vectors in ususal form. \n",
    "        vector = transform_tuple_into_vector( document_tuples[i], dictionary )\n",
    "        df_vector[i]= pd.DataFrame(vector)\n",
    "    return df_vector\n",
    "\n",
    "def get_close_documents(string, dataframe, dictionary, topn, path):\n",
    "    #string = unicode(string, encoding='utf-8', errors='replace').lower()\n",
    "    #print string\n",
    "    string = string_stemmer(string)\n",
    "    first_vector = transform_tuple_into_vector(dictionary.doc2bow(string.split()), dictionary )\n",
    "    #print first_vector\n",
    "    lengthlist = pd.Series()\n",
    "    for j in xrange(len(dataframe.columns)):\n",
    "        lengthlist = lengthlist.set_value( j, spatial.distance.cosine(first_vector, dataframe[j]))\n",
    "        lengthlist = lengthlist.sort_values().head(topn)\n",
    "        #now we have a topn index of the close documents. \n",
    "\n",
    "    namelist = pd.DataFrame(columns = ['name', 'content'])\n",
    "    namelist['distance'] = lengthlist\n",
    "    for i in lengthlist.index:\n",
    "        name, text = get_document_by_index(path,i)\n",
    "        namelist.set_value(i, 'name', name )\n",
    "        namelist.set_value(i, 'content', text)\n",
    "           \n",
    "    return namelist\n",
    "\n",
    "\n",
    "def get_document_by_index(path,index):\n",
    "    # this gets the filename and content of the document in a directory by index. \n",
    "    i= 0\n",
    "    for fname in os.listdir(path):\n",
    "        if i == index:\n",
    "            with open(os.path.join(path, fname)) as content_file:\n",
    "                content = content_file.read()  \n",
    "                content = unicode(content, encoding='utf-8', errors='replace')\n",
    "                return fname, content\n",
    "        i= i + 1\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def string_stemmer(line):\n",
    "    import re\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    line = re.sub(r'[^a-zA-Z ]',r'',line)\n",
    "    line = line.split()\n",
    "    line = [word for word in line if word not in stopwords.words('english')]  # remove the stop words. \n",
    "    output = []\n",
    "    for word in line:\n",
    "        output.append(stemmer.stem(word))     #stem all words \n",
    "    output = ' '.join(output)           # join the list to make a string\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stoplist = [u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours',\n",
    "             u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', \n",
    "             u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', \n",
    "             u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', \n",
    "             u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', \n",
    "             u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', \n",
    "             u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', \n",
    "             u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', \n",
    "             u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', \n",
    "             u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', \n",
    "             u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', \n",
    "             u'should', u'now']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get close document names for each question. \n",
    "\n",
    "#setup\n",
    "wiki_folder_path = '/Users/MK/GitHub/the_answer_is/data/wikipedia_stemmed_all_merged'\n",
    "train_file_path = '/Users/MK/GitHub/the_answer_is/data/training_set.tsv'\n",
    "dictionary_folder_path = '/Users/MK/GitHub/the_answer_is/data/temporary2'  #this folder contains one file which is stemmed. \n",
    "\n",
    "dictionary = get_dictionary(dictionary_folder_path)\n",
    "wiki_tuple = get_document_tuple( wiki_folder_path, dictionary )\n",
    "df_wiki_vector = transform_tuples_into_dataframe(wiki_tuple,dictionary)  \n",
    "train = pd.read_table(train_file_path,sep = '\\t')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 3\n",
    "close_document =  get_close_documents(train.ix[i][1], df_wiki_vector, dictionary, 5, wiki_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>content</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>distinct.txt_to_unicode_remove_stopwords_and_s...</td>\n",
       "      <td>two thing distinct two thing mathemat two thin...</td>\n",
       "      <td>0.613161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2415</th>\n",
       "      <td>epidemic.txt_to_unicode_remove_stopwords_and_s...</td>\n",
       "      <td>epidem greek epi upon demo peopl rapid spread ...</td>\n",
       "      <td>0.701917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918</th>\n",
       "      <td>kingdoms.txt_to_unicode_remove_stopwords_and_s...</td>\n",
       "      <td>pandem greek pan demo peopl epidem infecti dis...</td>\n",
       "      <td>0.715735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5131</th>\n",
       "      <td>pandemic.txt_to_unicode_remove_stopwords_and_s...</td>\n",
       "      <td>pandem greek pan demo peopl epidem infecti dis...</td>\n",
       "      <td>0.715735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5132</th>\n",
       "      <td>pandemics.txt_to_unicode_remove_stopwords_and_...</td>\n",
       "      <td>pandem greek pan demo peopl epidem infecti dis...</td>\n",
       "      <td>0.715735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   name  \\\n",
       "2006  distinct.txt_to_unicode_remove_stopwords_and_s...   \n",
       "2415  epidemic.txt_to_unicode_remove_stopwords_and_s...   \n",
       "3918  kingdoms.txt_to_unicode_remove_stopwords_and_s...   \n",
       "5131  pandemic.txt_to_unicode_remove_stopwords_and_s...   \n",
       "5132  pandemics.txt_to_unicode_remove_stopwords_and_...   \n",
       "\n",
       "                                                content  distance  \n",
       "2006  two thing distinct two thing mathemat two thin...  0.613161  \n",
       "2415  epidem greek epi upon demo peopl rapid spread ...  0.701917  \n",
       "3918  pandem greek pan demo peopl epidem infecti dis...  0.715735  \n",
       "5131  pandem greek pan demo peopl epidem infecti dis...  0.715735  \n",
       "5132  pandem greek pan demo peopl epidem infecti dis...  0.715735  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
