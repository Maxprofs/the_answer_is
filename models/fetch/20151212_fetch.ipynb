{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from gensim import corpora\n",
    "\n",
    "class MyDocument(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            with open(os.path.join(self.dirname, fname)) as content_file:\n",
    "                content = content_file.read()  \n",
    "                content = unicode(content, encoding='utf-8', errors='replace')\n",
    "                yield content.lower().split()\n",
    "\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                line = unicode(line, encoding='utf-8', errors='replace')\n",
    "                yield line.lower().split()\n",
    "\n",
    "def get_dictionary(path):\n",
    "    dictionary = corpora.Dictionary( MySentences(path) )\n",
    "    stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\n",
    "    dictionary.filter_tokens(stop_ids) # remove stop words and words that appear only once\n",
    "    dictionary.compactify() # remove gaps in id sequence after words that were removed\n",
    "    return dictionary\n",
    "\n",
    "def get_document_tuple( path, dictionary ):\n",
    "    vector = pd.Series()\n",
    "    doclist = MyDocument(path) \n",
    "    for i,content in enumerate(doclist):\n",
    "        vector = vector.set_value(i,dictionary.doc2bow(content))\n",
    "    return list(vector)\n",
    "\n",
    "def transform_tuple_into_vector(document_tuple,dictionary):\n",
    "    vector = pd.Series(np.zeros(len(dictionary.token2id)))        #0 array for storing wiki document vectors.\n",
    "    if not document_tuple == []:\n",
    "        for onetuple in document_tuple: \n",
    "            vector[onetuple[0]] = onetuple[1]\n",
    "        vector = vector / np.linalg.norm(vector, ord = 1)                       #normalize vector     \n",
    "        return vector \n",
    "    else:\n",
    "        return vector\n",
    "\n",
    "def transform_tuples_into_dataframe(document_tuples, dictionary):\n",
    "    length = len(document_tuples) \n",
    "    vector_initialization = pd.Series(np.zeros(len(dictionary.token2id)))  #0 array for storing wiki document vectors. \n",
    "    df_vector = pd.DataFrame(vector_initialization)  #initialize dataframe. all vectors will be stored. \n",
    "    for i in xrange(len(document_tuples)):           \n",
    "        #for each wiki documents, we will transform wiki vectors in tuple form into\n",
    "        #vectors in ususal form. \n",
    "        vector = transform_tuple_into_vector( document_tuples[i], dictionary )\n",
    "        df_vector[i]= pd.DataFrame(vector)\n",
    "    return df_vector\n",
    "\n",
    "def get_close_documents(string, dataframe, dictionary, topn, path):\n",
    "    #string = unicode(string, encoding='utf-8', errors='replace').lower()\n",
    "    #print string\n",
    "    string = string_stemmer(string)\n",
    "    first_vector = transform_tuple_into_vector(dictionary.doc2bow(string.split()), dictionary )\n",
    "    #print first_vector\n",
    "    lengthlist = pd.Series()\n",
    "    for j in xrange(len(dataframe.columns)):\n",
    "        lengthlist = lengthlist.set_value( j, spatial.distance.cosine(first_vector, dataframe[j]))\n",
    "        lengthlist = lengthlist.sort_values().head(topn)\n",
    "        #now we have a topn index of the close documents. \n",
    "\n",
    "    namelist = pd.DataFrame(columns = ['name', 'content'])\n",
    "    namelist['distance'] = lengthlist\n",
    "    for i in lengthlist.index:\n",
    "        name, text = get_document_by_index(path,i)\n",
    "        namelist.set_value(i, 'name', name )\n",
    "        namelist.set_value(i, 'content', text)\n",
    "           \n",
    "    return namelist\n",
    "\n",
    "\n",
    "def get_document_by_index(path,index):\n",
    "    # this gets the filename and content of the document in a directory by index. \n",
    "    i= 0\n",
    "    for fname in os.listdir(path):\n",
    "        if i == index:\n",
    "            with open(os.path.join(path, fname)) as content_file:\n",
    "                content = content_file.read()  \n",
    "                content = unicode(content, encoding='utf-8', errors='replace')\n",
    "                return fname, content\n",
    "        i= i + 1\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def string_stemmer(line):\n",
    "    import re\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    line = re.sub(r'[^a-zA-Z ]',r'',line)\n",
    "    line = line.split()\n",
    "    line = [word for word in line if word not in stopwords.words('english')]  # remove the stop words. \n",
    "    output = []\n",
    "    for word in line:\n",
    "        output.append(stemmer.stem(word))     #stem all words \n",
    "    output = ' '.join(output)           # join the list to make a string\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stoplist = [u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours',\n",
    "             u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', \n",
    "             u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', \n",
    "             u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', \n",
    "             u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', \n",
    "             u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', \n",
    "             u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', \n",
    "             u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', \n",
    "             u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', \n",
    "             u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', \n",
    "             u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', \n",
    "             u'should', u'now']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get close document names for each question. \n",
    "\n",
    "#setup\n",
    "wiki_folder_path = '/Users/MK/GitHub/the_answer_is/data/wikipedia_stemmed_all_merged'\n",
    "train_file_path = '/Users/MK/GitHub/the_answer_is/data/training_set.tsv'\n",
    "dictionary_folder_path = '/Users/MK/GitHub/the_answer_is/data/temporary2'  #this folder contains one file which is stemmed. \n",
    "\n",
    "def initialize_run_close_document(wiki_folder_path, dictionary_folder_path):\n",
    "    dictionary = get_dictionary(dictionary_folder_path)\n",
    "    wiki_tuple = get_document_tuple( wiki_folder_path, dictionary )\n",
    "    df_wiki_vector = transform_tuples_into_dataframe(wiki_tuple,dictionary) \n",
    "    return dictionary, df_wiki_vector\n",
    "\n",
    "def run_close_document( main_text , wiki_folder_path, dictionary, df_wiki_vector, topn):\n",
    "    close_document =  get_close_documents(main_text, df_wiki_vector, dictionary, topn, wiki_folder_path)\n",
    "    return close_document\n",
    "\n",
    "dictionary, df_wiki_vector = initialize_run_close_document(wiki_folder_path, dictionary_folder_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A family owns a vacation cabin located on a hillside below a gas station with a leaking gasoline storage tank. In which situation is the drinking water for the cabin most likely to be contaminated?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>content</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4015</th>\n",
       "      <td>leaking.txt_to_unicode_remove_stopwords_and_st...</td>\n",
       "      <td>leak way usual open fluid escap contain fluidc...</td>\n",
       "      <td>0.746988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090</th>\n",
       "      <td>drinking.txt_to_unicode_remove_stopwords_and_s...</td>\n",
       "      <td>drink act ingest water liquid bodi mouth water...</td>\n",
       "      <td>0.757191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   name  \\\n",
       "4015  leaking.txt_to_unicode_remove_stopwords_and_st...   \n",
       "2090  drinking.txt_to_unicode_remove_stopwords_and_s...   \n",
       "\n",
       "                                                content  distance  \n",
       "4015  leak way usual open fluid escap contain fluidc...  0.746988  \n",
       "2090  drink act ingest water liquid bodi mouth water...  0.757191  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_table('/Users/MK/GitHub/the_answer_is/data/training_set_which.tsv',sep = '\\t')\n",
    "close = run_close_document( train.ix[i][1] , wiki_folder_path, dictionary, df_wiki_vector, 2)\n",
    "print train.ix[i][1]\n",
    "close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
